\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb} % For mathematical symbols and equations
\usepackage{graphicx} % For including images
\usepackage{geometry} % To set margins
\usepackage{listings} % For code snippets
\usepackage{xcolor} % For colored text in code
\usepackage{hyperref} % For hyperlinks
\usepackage{caption} % For better captions
\usepackage{subcaption} % For subfigures

% Set margins
\geometry{margin=1in}

% Define code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{gray},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    frame=single,
    breaklines=true,
    breakatwhitespace=true
}

\title{Pattern Recognition Assignment: Decision Tree Classifier Implementation and Comparison}
\author{Your Name \\ Student ID: XXXXXXXX \\ University Name}
\date{March 10, 2025}

\begin{document}

% Title page
\maketitle

% Abstract
\begin{abstract}
This report presents the implementation of a custom Decision Tree Classifier in Python and compares its performance with scikit-learn's \texttt{DecisionTreeClassifier}. The custom implementation uses entropy and information gain as splitting criteria, with hyperparameters such as maximum depth and minimum samples per split. Experiments are conducted on the Iris dataset, and results are evaluated based on accuracy and tree structure. The report discusses implementation details, challenges, and differences between the two models.
\end{abstract}

% Table of contents
\tableofcontents
\newpage

\section{Introduction}
Pattern recognition involves developing algorithms to classify data based on learned patterns. Decision trees are a popular supervised learning method due to their interpretability and effectiveness in classification tasks. This assignment implements a custom Decision Tree Classifier from scratch and compares it with scikit-learn’s optimized implementation. The objectives are:
\begin{itemize}
    \item To understand the mechanics of decision tree construction.
    \item To evaluate performance differences between custom and library implementations.
    \item To visualize and interpret the resulting tree structures.
\end{itemize}

\section{Methodology}

\subsection{Dataset}
The Iris dataset \cite{fisher1936iris} is used, consisting of 150 samples with 4 features (sepal length, sepal width, petal length, petal width) and 3 classes (Setosa, Versicolor, Virginica). The dataset is split into 80\% training and 20\% testing sets using a random seed of 42 for reproducibility.

\subsection{Custom Decision Tree Implementation}
The custom \texttt{DecisionTreeClassifier} is implemented in Python with the following key components:
\begin{itemize}
    \item \textbf{Splitting Criterion}: Entropy \( H(y) = -\sum p_i \log_2(p_i) \) and information gain \( IG = H(\text{parent}) - \sum_{child} \frac{|child|}{|parent|} H(\text{child}) \).
    \item \textbf{Hyperparameters}: Maximum depth (\texttt{max\_depth=7}) and minimum samples to split (\texttt{min\_samples\_split=10}).
    \item \textbf{Tree Construction}: Recursive splitting based on the best feature and threshold.
\end{itemize}

A snippet of the splitting logic is shown below:
\begin{lstlisting}
def get_best_split(self, X, y):
    best_split = {'info_gain': float('-inf'), 'threshold': None, 'feature_index': None}
    for feature_index in range(X.shape[1]):
        thresholds = np.unique(X[:, feature_index])
        for threshold in thresholds:
            left_indices = X[:, feature_index] <= threshold
            right_indices = X[:, feature_index] > threshold
            split_info_gain = self.compute_information_gain(y, y[left_indices], y[right_indices])
            if split_info_gain > best_split['info_gain']:
                best_split['info_gain'] = split_info_gain
                best_split['feature_index'] = feature_index
                best_split['threshold'] = threshold
    return best_split
\end{lstlisting}

\subsection{Scikit-learn Implementation}
The scikit-learn \texttt{DecisionTreeClassifier} is configured with identical hyperparameters (\texttt{max\_depth=7}, \texttt{min\_samples\_split=10}, \texttt{criterion='entropy'}, \texttt{random\_state=42}) to ensure a fair comparison.

\section{Experiments and Results}

\subsection{Experimental Setup}
Both models were trained on the Iris training set and evaluated on the test set. Accuracy was computed as the fraction of correctly predicted labels. The tree structures were visualized and compared.

\subsection{Results}
The custom implementation achieved an accuracy of \textbf{XX.XX\%}, while scikit-learn’s model achieved \textbf{YY.YY\%}. The tree structures are shown below:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{sklearn_tree.png} % Replace with your image file
    \caption{Scikit-learn Decision Tree Visualization}
    \label{fig:sklearn_tree}
\end{figure}

The custom tree structure (printed textually) is as follows:
\begin{verbatim}
X[2] <= 2.45 -> True:
 Predict: 0
X[2] > 2.45 -> False:
 X[3] <= 1.75 -> True:
  X[2] <= 4.95 -> True:
   Predict: 1
  X[2] > 4.95 -> False:
   Predict: 2
 X[3] > 1.75 -> False:
  Predict: 2
\end{verbatim}

\subsection{Analysis}
The accuracy difference may stem from:
\begin{itemize}
    \item \textbf{Split Selection}: The custom implementation selects the last best split in case of ties, while scikit-learn uses a deterministic tie-breaking mechanism.
    \item \textbf{Numerical Precision}: Scikit-learn’s optimized entropy calculations may handle edge cases better.
\end{itemize}

\section{Discussion}
The custom implementation successfully replicates core decision tree functionality but lacks optimizations present in scikit-learn, such as pruning or handling of numerical instability. Visualization issues (e.g., GraphViz errors) were resolved by installing necessary dependencies.

\section{Conclusion}
This assignment demonstrated the construction and evaluation of a decision tree classifier. While the custom model performs comparably to scikit-learn, differences in accuracy highlight the importance of implementation details. Future work could include adding pruning or Gini impurity as an alternative criterion.

% Bibliography
\begin{thebibliography}{9}
\bibitem{fisher1936iris}
Fisher, R.A.,
``The Use of Multiple Measurements in Taxonomic Problems,''
\textit{Annals of Eugenics}, 7(2), 179--188, 1936.
\end{thebibliography}

\end{document}