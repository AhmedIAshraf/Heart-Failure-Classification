{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baed3231",
   "metadata": {},
   "source": [
    "# Heart Failure Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be1512-2afb-46ff-a41f-e0f07bb5109d",
   "metadata": {},
   "source": [
    "## 1. Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27da61f-d2f6-4933-97cd-8000f8fa8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from math import log2\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ca0ba-7190-41f3-8a16-c9095c0306c7",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477f425",
   "metadata": {},
   "source": [
    "#### 2.1 Loading Heart Failure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5615a-9001-4a11-9084-619e731489ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "heart_df = pd.read_csv('heart.csv')\n",
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb3138",
   "metadata": {},
   "source": [
    "#### 2.2 Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using One-Hot Encoding\n",
    "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_features = encoder.fit_transform(heart_df[categorical_features])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "modified_df = pd.concat([heart_df.drop(categorical_features, axis=1), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0cec0",
   "metadata": {},
   "source": [
    "#### 2.3 Splitting Data to Training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X = modified_df.drop('HeartDisease', axis=1)\n",
    "y = modified_df['HeartDisease']\n",
    "\n",
    "# Split the dataset into training, validation (80%) and test (20%) sets, maintaining class distribution\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split the training set into training (70%) and validation (10%) sets, maintaining class distribution\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "print(np.sum(y_train == 1) / len(y_train))\n",
    "print(np.sum(y_val == 1) / len(y_val))\n",
    "print(np.sum(y_test == 1) / len(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e0d70",
   "metadata": {},
   "source": [
    "#### 2.4 Standardizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70164cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6793a8",
   "metadata": {},
   "source": [
    "# 3. Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8dbc6d",
   "metadata": {},
   "source": [
    "## 3.1 Decision Tree Classifier Implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9e9bc",
   "metadata": {},
   "source": [
    "### Tree Node Class\n",
    "The `Node` class represents a single node in a decision tree structure. It is used to store information about a decision point (for internal nodes) or a predicted value (for leaf nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self, left=None, right=None, info_gain=None, feature_index=None, threshold=None, value=None):\n",
    "    self.left = left\n",
    "    self.right = right\n",
    "    self.info_gain = info_gain  # Information gain of split\n",
    "    self.feature_index = feature_index  # Index of feature to split on\n",
    "    self.threshold = threshold  # Threshold value to split on\n",
    "    self.value = value  # Predicted value if node is leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76298d5",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Class\n",
    "The `DecisionTreeClassifier` class implements a basic decision tree classifier. It recursively splits the feature space based on information gain, building a binary tree structure to classify data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc95340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=7, min_samples_split=10):\n",
    "        \"\"\"Initialize the decision tree with hyperparameters.\"\"\"\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree\n",
    "        self.min_samples_split = min_samples_split  # Minimum samples required to split a node\n",
    "        self.root = None  # Root node of the tree, starts as None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree using the input features X and target y.\"\"\"\n",
    "        self.root = self.build_tree(X, y)  # Build the tree and set the root\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for each sample in X.\"\"\"\n",
    "        if self.root is None:\n",
    "            raise ValueError(\"Model has not been trained. Please call 'fit' method first.\")\n",
    "        # Return predictions by traversing the tree for each sample\n",
    "        return [self.make_prediction(x, self.root) for x in X]\n",
    "\n",
    "    def make_prediction(self, x, node):\n",
    "        \"\"\"Recursively traverse the tree to make a prediction for a single sample.\"\"\"\n",
    "        if node.value is not None:  # Leaf node reached\n",
    "            return node.value\n",
    "        # Decide which subtree to follow based on the feature value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self.make_prediction(x, node.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, node.right)\n",
    "\n",
    "    def print_tree(self, feature_names):\n",
    "        \"\"\"Print the structure of the decision tree.\"\"\"\n",
    "        self.print_tree_helper(self.root, 0, feature_names)  # Start from the root with depth 0\n",
    "\n",
    "    def print_tree_helper(self, node, depth, feature_names):\n",
    "        \"\"\"Helper method to recursively print the tree with indentation.\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "        indent = \" \" * depth  # Indentation for visual hierarchy\n",
    "        if node.value is not None:  # Leaf node\n",
    "            print(f\"{indent}Predict: {node.value}\")\n",
    "        else:  # Internal node\n",
    "            print(f\"{indent}{feature_names[node.feature_index]} <= {node.threshold} -> True:\")\n",
    "            self.print_tree_helper(node.left, depth + 1, feature_names)\n",
    "            print(f\"{indent}{feature_names[node.feature_index]} > {node.threshold} -> False:\")\n",
    "            self.print_tree_helper(node.right, depth + 1, feature_names)\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        m, n = X.shape  # Number of samples (m) and features (n)\n",
    "        is_pure = len(np.unique(y)) == 1  # Check if all labels are the same\n",
    "\n",
    "        # Base case: stop if max depth reached, data is pure, or too few samples\n",
    "        if depth >= self.max_depth or is_pure or m < self.min_samples_split:\n",
    "            majority_class = self.compute_output(y)  # Assign majority class to leaf\n",
    "            return Node(value=majority_class)\n",
    "\n",
    "        # Find the best split and recursively build left and right subtrees\n",
    "        best_split = self.get_best_split(X, y)\n",
    "        left_indices = best_split['left']\n",
    "        right_indices = best_split['right']\n",
    "        left_subtree = self.build_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
    "        right_subtree = self.build_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
    "\n",
    "        # Return a node with the split details and subtrees\n",
    "        return Node(left=left_subtree,\n",
    "                    right=right_subtree,\n",
    "                    info_gain=best_split['info_gain'],\n",
    "                    feature_index=best_split['feature_index'],\n",
    "                    threshold=best_split['threshold'])\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        \"\"\"Find the best feature and threshold to split the data.\"\"\"\n",
    "        _, n = X.shape\n",
    "        best_split = {\n",
    "            'info_gain': float('-inf'),\n",
    "            'threshold': None,\n",
    "            'feature_index': None,\n",
    "            'left': None,\n",
    "            'right': None\n",
    "        }\n",
    "\n",
    "        # Iterate over all features and their unique values as potential thresholds\n",
    "        for feature_index in range(n):\n",
    "            feature_values = X[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                # Split data based on the threshold\n",
    "                left_indices = X[:, feature_index] <= threshold\n",
    "                right_indices = X[:, feature_index] > threshold\n",
    "                left_y = y[left_indices]\n",
    "                right_y = y[right_indices]\n",
    "\n",
    "                if len(left_y) == 0 or len(right_y) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain for this split\n",
    "                split_info_gain = self.compute_information_gain(y, left_y, right_y)\n",
    "\n",
    "                # Update best split if this one is better\n",
    "                if split_info_gain > best_split['info_gain']:\n",
    "                    best_split['info_gain'] = split_info_gain\n",
    "                    best_split['feature_index'] = feature_index\n",
    "                    best_split['threshold'] = threshold\n",
    "                    best_split['left'] = left_indices\n",
    "                    best_split['right'] = right_indices\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def compute_output(self, y):\n",
    "        \"\"\"Compute the majority class for a leaf node.\"\"\"\n",
    "        unique_classes, frequency = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(frequency)]  # Most frequent class\n",
    "        # majority_count = frequency.max()\n",
    "        # print(f\"\\nMajority Class: {majority_class} (Count: {majority_count})\")\n",
    "        return int(majority_class)\n",
    "\n",
    "    def compute_entropy(self, y):\n",
    "        \"\"\"Calculate the entropy of a set of labels.\"\"\"\n",
    "        unique_classes, frequencies = np.unique(y, return_counts=True)\n",
    "        n_classes = len(y)\n",
    "        entropy = 0\n",
    "        for frequency in frequencies:\n",
    "            probability = frequency / n_classes\n",
    "            entropy -= probability * log2(probability) if probability > 0 else 0\n",
    "        return entropy\n",
    "\n",
    "    def compute_information_gain(self, y, left_y, right_y):\n",
    "        \"\"\"Calculate information gain from a split.\"\"\"\n",
    "        m = len(y)\n",
    "        left_m = len(left_y)\n",
    "        right_m = len(right_y)\n",
    "\n",
    "        # Entropy before and after the split\n",
    "        parent_entropy = self.compute_entropy(y)\n",
    "        left_entropy = self.compute_entropy(left_y) if left_m > 0 else 0\n",
    "        right_entropy = self.compute_entropy(right_y) if right_m > 0 else 0\n",
    "\n",
    "        # Weighted average of child entropies\n",
    "        weighted_entropy = (left_m / m) * left_entropy + (right_m / m) * right_entropy\n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55e8de",
   "metadata": {},
   "source": [
    "### Training the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770784ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Decision Tree Classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "decision_tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406e312",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm):\n",
    "    \"\"\"Plot the confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar(label='Count')\n",
    "    plt.xticks([0, 1], ['Predicted 0', 'Predicted 1'])\n",
    "    plt.yticks([0, 1], ['True 0', 'True 1'])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "max_depth_values = [2, 3, 5, 7, 9, 11, 13, 15]\n",
    "min_samples_split_values = [2, 5, 10, 15, 20, 25, 30]\n",
    "misclassification_rates = np.zeros((len(max_depth_values), len(min_samples_split_values)))\n",
    "\n",
    "for i, max_depth in enumerate(max_depth_values):\n",
    "    for j, min_samples_split in enumerate(min_samples_split_values):\n",
    "        decision_tree_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        decision_tree_classifier.fit(X_train, y_train)\n",
    "        y_pred = decision_tree_classifier.predict(X_val)\n",
    "        # cm = confusion_matrix(y_val, y_pred)\n",
    "        # plot_confusion_matrix(cm)\n",
    "        misclassification_rates[i, j] = np.mean(y_pred != y_val)\n",
    "        print(f\"Misclassification Rate (Depth={max_depth}, MinSamplesSplit={min_samples_split}): {misclassification_rates[i, j]}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f98990",
   "metadata": {},
   "source": [
    "### Training the Final Model with Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45932a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best hyperparameters\n",
    "best_idx = np.unravel_index(np.argmin(misclassification_rates, axis=None), misclassification_rates.shape)\n",
    "best_max_depth = max_depth_values[best_idx[0]]\n",
    "best_min_samples_split = min_samples_split_values[best_idx[1]]\n",
    "print(f\"Best Max Depth: {best_max_depth}\")\n",
    "print(f\"Best Min Samples Split: {best_min_samples_split}\")\n",
    "\n",
    "# Intialize and train the Decision Tree Classifier with the best hyperparameters\n",
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_split=best_min_samples_split)\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model on the test set\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the misclassification rate, accuracy, F1-score, and confusion matrix\n",
    "misclassification_rate = np.mean(y_pred != y_test)\n",
    "print(f\"Misclassification Rate: {misclassification_rate}\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Print the structure of the decision tree\n",
    "decision_tree_classifier.print_tree(feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24023375",
   "metadata": {},
   "source": [
    "## 3.2 Bagging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d149a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94873570",
   "metadata": {},
   "source": [
    "## 3.3 AdaBoost Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d4539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa95c651",
   "metadata": {},
   "source": [
    "# 4. Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d2452",
   "metadata": {},
   "source": [
    "## 4.1 K-Nearest Neighbors (KNN) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4a466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fabea6f2",
   "metadata": {},
   "source": [
    "## 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b4a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "699ecf69",
   "metadata": {},
   "source": [
    "## 4.3 Feedforward Neural Network (FNN) with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f229a37",
   "metadata": {},
   "source": [
    "### Building the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add the hidden layer with ReLU activation function\n",
    "network.add(layers.Dense(16, activation='relu', input_shape=(X.shape[1],)))\n",
    "\n",
    "# Add the output layer with sigmoid activation function\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Add SGD optimizer with learning rate 0.01\n",
    "optimizer = optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the network with binary cross-entropy loss function\n",
    "network.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the network\n",
    "history = network.fit(X_train_scaled, \n",
    "                      y_train,\n",
    "                      epochs=50,\n",
    "                      verbose=1)\n",
    "\n",
    "# Evaluate the network\n",
    "test_loss, test_accuracy = network.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test loss before tuning: {test_loss}')\n",
    "print(f'Test accuracy before tuning: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab347ec",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the hyperparameters\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_units = [8, 16, 32]\n",
    "epochs = [50, 100, 200, 500]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_neurons in hidden_units:\n",
    "        for n_epochs in epochs:\n",
    "            network = models.Sequential()\n",
    "            network.add(layers.Dense(n_neurons, activation='relu', input_shape=(X.shape[1],)))\n",
    "            network.add(layers.Dense(1, activation='sigmoid'))\n",
    "            optimizer = optimizers.SGD(learning_rate=lr)\n",
    "            network.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "            # Early stopping\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "            history = network.fit(X_train_scaled, \n",
    "                                    y_train, \n",
    "                                    epochs=n_epochs, \n",
    "                                    verbose=0, \n",
    "                                    callbacks=[early_stopping],\n",
    "                                    validation_data=(X_val_scaled, y_val))\n",
    "            val_loss, val_accuracy = network.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "            print(f'lr={lr}, n_neurons={n_neurons}, n_epochs={n_epochs}, val_accuracy={val_accuracy}')\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_hyperparameters = (lr, n_neurons, n_epochs)\n",
    "\n",
    "print(f\"Best hyperparameters: lr={best_hyperparameters[0]}, n_neurons={best_hyperparameters[1]}, n_epochs={best_hyperparameters[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a34dea",
   "metadata": {},
   "source": [
    "### Training the Final Model with Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f570f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add the hidden layer with ReLU activation function\n",
    "network.add(layers.Dense(best_hyperparameters[1], activation='relu', input_shape=(X.shape[1],)))\n",
    "\n",
    "# Add the output layer with sigmoid activation function\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Add SGD optimizer with learning rate 0.01\n",
    "optimizer = optimizers.SGD(learning_rate=best_hyperparameters[0])\n",
    "\n",
    "# Compile the network with binary cross-entropy loss function\n",
    "network.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the network\n",
    "history = network.fit(X_train_scaled, \n",
    "                      y_train,\n",
    "                      epochs=best_hyperparameters[2],\n",
    "                      verbose=1,\n",
    "                      callbacks=[early_stopping],\n",
    "                      validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Evaluate the network\n",
    "test_loss, test_accuracy = network.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test loss after tuning: {test_loss}')\n",
    "print(f'Test accuracy after tuning: {test_accuracy}')\n",
    "\n",
    "y_pred_prob = network.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 score: {f1:.4f}\")\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
